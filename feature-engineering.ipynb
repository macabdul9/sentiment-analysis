{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = brown.sents(categories='adventure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Dan', 'Morgan', 'told', 'himself', 'he', 'would', 'forget', 'Ann', 'Turner', '.'], ['He', 'was', 'well', 'rid', 'of', 'her', '.'], ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-8cbabfbcfb86>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-8cbabfbcfb86>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    for i in\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for i in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'What are you doing ? We are studying machine learning . It is interesting field. Is it ?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What are you doing ?', 'We are studying machine learning .', 'It is interesting field.', 'Is it ?']\n"
     ]
    }
   ],
   "source": [
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What', 'are', 'you', 'doing', '?']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"Email me all the 10 assignments of Lecture 0, 1, 2 at xyz@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = '[a-zA-Z]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = RegexpTokenizer(regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = tokenizer.tokenize(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"needn't\", 'those', 'same', 'mustn', 'very', 'yourselves', 'below', 'down', 'or', 'from', 'haven', \"shan't\", \"don't\", 'had', 'were', \"didn't\", 'yourself', \"she's\", 'should', 'having', \"mightn't\", 'to', 'if', \"hadn't\", \"weren't\", 'don', 'a', 'any', 'own', 'm', 'now', 'above', 'these', 'have', \"shouldn't\", 'what', 'has', 'here', \"you'd\", 'again', 'then', 'couldn', 'hasn', 'where', 'and', 'itself', 'of', 'her', 'will', \"hasn't\", 'wasn', 'how', 'after', 'both', 'between', 'yours', 'are', 'while', 'under', 'won', 'over', 'out', 'am', 'him', 'themselves', 'herself', 'ours', 'needn', 'his', 'who', 'into', 's', 'was', 'do', 'all', 'as', 'aren', 'my', 'about', 'than', 'himself', \"isn't\", 'too', 'i', 'during', \"should've\", \"wouldn't\", 'against', 'nor', 'up', 'few', 'that', 'being', 'just', 're', 've', 'off', 'such', 'weren', 'why', 'y', 'is', 'further', 'll', 'which', 'an', 'not', 'it', 'myself', 'wouldn', 'at', 'mightn', 'they', \"mustn't\", \"it's\", \"that'll\", 'because', 'before', 'does', 'be', \"doesn't\", 'hadn', 'isn', 'until', 'ma', \"you've\", 'been', 'you', \"aren't\", 'some', 'on', 'can', 'doesn', \"you'll\", \"wasn't\", 'in', 'their', \"couldn't\", 'theirs', 'through', 'ain', 'its', 'more', 'the', 't', 'hers', 'didn', \"you're\", 'shan', 'whom', 'me', 'when', 'so', 'he', 'we', 'did', 'ourselves', 'your', 'doing', \"won't\", 'this', 'each', 'she', 'other', \"haven't\", 'once', 'for', 'd', 'but', 'there', 'most', 'only', 'them', 'by', 'shouldn', 'o', 'no', 'with', 'our'}\n"
     ]
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E',\n",
       " 'm',\n",
       " 'a',\n",
       " 'i',\n",
       " 'l',\n",
       " 'm',\n",
       " 'e',\n",
       " 'a',\n",
       " 'l',\n",
       " 'l',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " 'a',\n",
       " 's',\n",
       " 's',\n",
       " 'i',\n",
       " 'g',\n",
       " 'n',\n",
       " 'm',\n",
       " 'e',\n",
       " 'n',\n",
       " 't',\n",
       " 's',\n",
       " 'o',\n",
       " 'f',\n",
       " 'L',\n",
       " 'e',\n",
       " 'c',\n",
       " 't',\n",
       " 'u',\n",
       " 'r',\n",
       " 'e',\n",
       " 'a',\n",
       " 't',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " 'g',\n",
       " 'm',\n",
       " 'a',\n",
       " 'i',\n",
       " 'l',\n",
       " 'c',\n",
       " 'o',\n",
       " 'm']"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sent, stopwords):\n",
    "    useful_words = [w for w in sent if w not in stopwords]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful = remove_stopwords(sample, stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E',\n",
       " 'l',\n",
       " 'e',\n",
       " 'l',\n",
       " 'l',\n",
       " 'h',\n",
       " 'e',\n",
       " 'g',\n",
       " 'n',\n",
       " 'e',\n",
       " 'n',\n",
       " 'f',\n",
       " 'L',\n",
       " 'e',\n",
       " 'c',\n",
       " 'u',\n",
       " 'r',\n",
       " 'e',\n",
       " 'x',\n",
       " 'z',\n",
       " 'g',\n",
       " 'l',\n",
       " 'c']"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "sb =  SnowballStemmer(language='english')\n",
    "ls = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('having')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb.stem('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb.stem('having')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hav'"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls.stem('having')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eating'"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemmatize('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'story'"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemmatize('stories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['Before the prorogation, however, he saw the invaluable Act of Habeas Corpus, which he had carried through parliament, receive the royal assent.',\n",
    "    'The scriptures were a corpus of texts held together in one giant collection of writing.',\n",
    "    'In the bottom of the writer’s desk, a corpus of never published manuscripts was found.',\n",
    "    'The corpus contained many different articles written by the author shortly before his death in 1998']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(RegexpTokenizer('[a-zA-Z]+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 2 0 0 1 0 1 0 0 0 1 0 1 1 0 1 1 1\n",
      "  0 0 0 3 1 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 2 1 0 0 0 0 0 0\n",
      "  1 0 1 1 0 1 0 1 0 0 1 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 2 0 0 0 1 0 0 0\n",
      "  0 0 0 2 0 0 1 0 0 1 0 0]\n",
      " [1 0 1 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0\n",
      "  0 1 0 2 0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'before': 5, 'the': 39, 'prorogation': 31, 'however': 22, 'he': 19, 'saw': 35, 'invaluable': 24, 'act': 1, 'of': 28, 'habeas': 17, 'corpus': 11, 'which': 44, 'had': 18, 'carried': 8, 'through': 40, 'parliament': 30, 'receive': 33, 'royal': 34, 'assent': 3, 'scriptures': 36, 'were': 43, 'texts': 38, 'held': 20, 'together': 41, 'in': 23, 'one': 29, 'giant': 16, 'collection': 9, 'writing': 46, 'bottom': 6, 'writer': 45, 'desk': 13, 'never': 27, 'published': 32, 'manuscripts': 25, 'was': 42, 'found': 15, 'contained': 10, 'many': 26, 'different': 14, 'articles': 2, 'written': 47, 'by': 7, 'author': 4, 'shortly': 37, 'his': 21, 'death': 12, '1998': 0}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 2 0 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(arr[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['1998', 'articles', 'author', 'before', 'by', 'contained',\n",
       "        'corpus', 'death', 'different', 'his', 'in', 'many', 'shortly',\n",
       "        'the', 'written'], dtype='<U11')]"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.inverse_transform(arr[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.21441172 0.         0.21441172 0.         0.16904466\n",
      "  0.         0.         0.21441172 0.         0.         0.11188893\n",
      "  0.         0.         0.         0.         0.         0.21441172\n",
      "  0.21441172 0.42882344 0.         0.         0.21441172 0.\n",
      "  0.21441172 0.         0.         0.         0.13685622 0.\n",
      "  0.21441172 0.21441172 0.         0.21441172 0.21441172 0.21441172\n",
      "  0.         0.         0.         0.33566679 0.21441172 0.\n",
      "  0.         0.         0.21441172 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.29384212 0.         0.15333901\n",
      "  0.         0.         0.         0.         0.29384212 0.\n",
      "  0.         0.         0.29384212 0.         0.         0.18755562\n",
      "  0.         0.         0.         0.         0.37511124 0.29384212\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.29384212 0.         0.29384212 0.15333901 0.         0.29384212\n",
      "  0.         0.29384212 0.         0.         0.29384212 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.29619205 0.         0.         0.         0.         0.1545653\n",
      "  0.         0.29619205 0.         0.29619205 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.18905555\n",
      "  0.         0.29619205 0.         0.29619205 0.3781111  0.\n",
      "  0.         0.         0.29619205 0.         0.         0.\n",
      "  0.         0.         0.         0.3091306  0.         0.\n",
      "  0.29619205 0.         0.         0.29619205 0.         0.        ]\n",
      " [0.27327509 0.         0.27327509 0.         0.27327509 0.21545322\n",
      "  0.         0.27327509 0.         0.         0.27327509 0.14260628\n",
      "  0.27327509 0.         0.27327509 0.         0.         0.\n",
      "  0.         0.         0.         0.27327509 0.         0.17442795\n",
      "  0.         0.         0.27327509 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.27327509 0.         0.28521256 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.27327509]]\n"
     ]
    }
   ],
   "source": [
    "vd = tf.fit_transform(corpus).toarray()\n",
    "print(vd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'before': 5,\n",
       " 'the': 39,\n",
       " 'prorogation': 31,\n",
       " 'however': 22,\n",
       " 'he': 19,\n",
       " 'saw': 35,\n",
       " 'invaluable': 24,\n",
       " 'act': 1,\n",
       " 'of': 28,\n",
       " 'habeas': 17,\n",
       " 'corpus': 11,\n",
       " 'which': 44,\n",
       " 'had': 18,\n",
       " 'carried': 8,\n",
       " 'through': 40,\n",
       " 'parliament': 30,\n",
       " 'receive': 33,\n",
       " 'royal': 34,\n",
       " 'assent': 3,\n",
       " 'scriptures': 36,\n",
       " 'were': 43,\n",
       " 'texts': 38,\n",
       " 'held': 20,\n",
       " 'together': 41,\n",
       " 'in': 23,\n",
       " 'one': 29,\n",
       " 'giant': 16,\n",
       " 'collection': 9,\n",
       " 'writing': 46,\n",
       " 'bottom': 6,\n",
       " 'writer': 45,\n",
       " 'desk': 13,\n",
       " 'never': 27,\n",
       " 'published': 32,\n",
       " 'manuscripts': 25,\n",
       " 'was': 42,\n",
       " 'found': 15,\n",
       " 'contained': 10,\n",
       " 'many': 26,\n",
       " 'different': 14,\n",
       " 'articles': 2,\n",
       " 'written': 47,\n",
       " 'by': 7,\n",
       " 'author': 4,\n",
       " 'shortly': 37,\n",
       " 'his': 21,\n",
       " 'death': 12,\n",
       " '1998': 0}"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ng.data\n",
    "Y = ng.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
